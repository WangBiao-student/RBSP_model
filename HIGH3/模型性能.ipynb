{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "# 完全忽略PerformanceWarning\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_df=pd.read_hdf('combined_df_train.h5', key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m flux_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1.1\u001b[39m, \u001b[38;5;241m6.6\u001b[39m, \u001b[38;5;241m0.1\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     combined_df_i \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInterpolated_Lstar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m     combined_df_i \u001b[38;5;241m=\u001b[39m combined_df_i[combined_df_i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInterpolated_Lstar\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#计算范围内的Flux均值\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py:3884\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3882\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3886\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3887\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3888\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py:3946\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3943\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   3945\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 3946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\generic.py:4088\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4077\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4079\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4080\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4081\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4086\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4087\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4088\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\generic.py:4068\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4063\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4064\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4065\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4066\u001b[0m     )\n\u001b[1;32m-> 4068\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4070\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4072\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4074\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4075\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals\\managers.py:877\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    874\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    876\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals\\managers.py:670\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    663\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    664\u001b[0m         indexer,\n\u001b[0;32m    665\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    666\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    667\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    668\u001b[0m     )\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    671\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    672\u001b[0m             indexer,\n\u001b[0;32m    673\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    674\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    675\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    676\u001b[0m             ),\n\u001b[0;32m    677\u001b[0m         )\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    679\u001b[0m     ]\n\u001b[0;32m    681\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    682\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals\\managers.py:671\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    663\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    664\u001b[0m         indexer,\n\u001b[0;32m    665\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    666\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    667\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    668\u001b[0m     )\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    670\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 671\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    679\u001b[0m     ]\n\u001b[0;32m    681\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    682\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1061\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1061\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:118\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    117\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "combined_df=combined_df[combined_df['Kp_0h'] <30]\n",
    "flux_dict = {}\n",
    "for i in np.arange(1.1, 6.6, 0.1):\n",
    "    combined_df_i = combined_df[combined_df['Interpolated_Lstar'] > i]\n",
    "    combined_df_i = combined_df_i[combined_df_i['Interpolated_Lstar'] < i + 0.1]\n",
    "    #计算范围内的Flux均值\n",
    "    flux_mean = combined_df_i['Flux'].mean()\n",
    "    #将均值存入字典中\n",
    "    flux_dict[i] = flux_mean\n",
    "#将flux_dict保存为csv文件\n",
    "flux_df = pd.DataFrame(list(flux_dict.items()), columns=['Interpolated_Lstar', 'Flux'])\n",
    "print(flux_df)\n",
    "flux_df.to_csv('flux_L_2280.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Interpolated_Lstar        Flux\n",
      "0                  1.1    0.029579\n",
      "1                  1.2    0.059217\n",
      "2                  1.3    0.103723\n",
      "3                  1.4    0.131334\n",
      "4                  1.5    0.080401\n",
      "5                  1.6    0.062091\n",
      "6                  1.7    0.021432\n",
      "7                  1.8    0.019994\n",
      "8                  1.9    0.021522\n",
      "9                  2.0    0.031850\n",
      "10                 2.1    0.049134\n",
      "11                 2.2    0.061982\n",
      "12                 2.3    0.054787\n",
      "13                 2.4    0.050247\n",
      "14                 2.5    0.043466\n",
      "15                 2.6    0.065674\n",
      "16                 2.7    0.203784\n",
      "17                 2.8    0.748988\n",
      "18                 2.9    2.088719\n",
      "19                 3.0    4.886664\n",
      "20                 3.1   10.973515\n",
      "21                 3.2   19.521433\n",
      "22                 3.3   34.810065\n",
      "23                 3.4   49.205588\n",
      "24                 3.5   68.091771\n",
      "25                 3.6   83.676846\n",
      "26                 3.7   98.075017\n",
      "27                 3.8  108.250354\n",
      "28                 3.9  111.622987\n",
      "29                 4.0  114.891598\n",
      "30                 4.1  113.172441\n",
      "31                 4.2  106.103635\n",
      "32                 4.3   98.613847\n",
      "33                 4.4   91.493223\n",
      "34                 4.5   81.009100\n",
      "35                 4.6   70.757435\n",
      "36                 4.7   59.581617\n",
      "37                 4.8   48.592840\n",
      "38                 4.9   38.853473\n",
      "39                 5.0   31.366835\n",
      "40                 5.1   23.733052\n",
      "41                 5.2   18.874673\n",
      "42                 5.3   15.388488\n",
      "43                 5.4   13.174471\n",
      "44                 5.5   10.171044\n",
      "45                 5.6    7.344203\n",
      "46                 5.7    5.175387\n",
      "47                 5.8    3.393715\n",
      "48                 5.9    2.510586\n",
      "49                 6.0    1.416770\n",
      "50                 6.1    1.438701\n",
      "51                 6.2    2.155113\n",
      "52                 6.3    1.588272\n",
      "53                 6.4    0.783859\n",
      "54                 6.5    0.000000\n"
     ]
    }
   ],
   "source": [
    "flux_dict = {}\n",
    "for i in np.arange(1.1, 6.6, 0.1):\n",
    "    combined_df_i = combined_df[combined_df['Interpolated_Lstar'] > i]\n",
    "    combined_df_i = combined_df_i[combined_df_i['Interpolated_Lstar'] < i + 0.1]\n",
    "    #计算范围内的Flux均值\n",
    "    flux_mean = combined_df_i['Flux'].mean()\n",
    "    #将均值存入字典中\n",
    "    flux_dict[i] = flux_mean\n",
    "#将flux_dict保存为csv文件\n",
    "flux_df = pd.DataFrame(list(flux_dict.items()), columns=['Interpolated_Lstar', 'Flux'])\n",
    "print(flux_df)\n",
    "flux_df.to_csv('flux_L_2280.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=150\n",
    "VX_time1 = [f'VX_{i}h' for i in range(0, time_steps + 1)]\n",
    "SYMH_time2 = [f'SYMH_{i}h' for i in range(0, time_steps + 1)]\n",
    "VX_IS1_time3 = [f'VX_IS1_{i}h' for i in range(0, time_steps + 1)]\n",
    "BZGSM_time4 = [f'BZGSM_{i}h' for i in range(0, time_steps + 1)]\n",
    "TEMP_time5 = [f'TEMP_{i}h' for i in range(0, time_steps + 1)]\n",
    "PDYN_time6 = [f'PDYN_{i}h' for i in range(0, time_steps + 1)]\n",
    "Kp_time7 = [f'Kp_{i}h' for i in range(0, time_steps + 1)]\n",
    "Dst_time8 = [f'Dst_{i}h' for i in range(0, time_steps + 1)]\n",
    "F107_time9 = [f'F107_{i}h' for i in range(0, time_steps + 1)]\n",
    "AE_time10 = [f'AE_{i}h' for i in range(0, time_steps + 1)]\n",
    "AL_time11 = [f'AL_{i}h' for i in range(0, time_steps + 1)]\n",
    "AU_time12 = [f'AU_{i}h' for i in range(0, time_steps + 1)]\n",
    "BZGSM_IS1_time13 = [f'BZGSM_IS1_{i}h' for i in range(0, time_steps + 1)]\n",
    "TEMP_IS1_time14 = [f'TEMP_IS1_{i}h' for i in range(0, time_steps + 1)]\n",
    "PDYN_IS1_time15 = [f'PDYN_IS1_{i}h' for i in range(0, time_steps + 1)]\n",
    "F107_IS1_time16 = [f'F107_IS1_{i}h' for i in range(0, time_steps + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1231821, 151, 16)\n"
     ]
    }
   ],
   "source": [
    "X_time1 = combined_df[VX_time1].values  \n",
    "X_time2 = combined_df[SYMH_time2].values \n",
    "X_time3 = combined_df[VX_IS1_time3].values\n",
    "X_time4 = combined_df[BZGSM_time4].values\n",
    "X_time5 = combined_df[TEMP_time5].values\n",
    "X_time6 = combined_df[PDYN_time6].values\n",
    "X_time7 = combined_df[Kp_time7].values\n",
    "X_time8 = combined_df[Dst_time8].values\n",
    "X_time9 = combined_df[F107_time9].values\n",
    "X_time10 = combined_df[AE_time10].values\n",
    "X_time11 = combined_df[AL_time11].values\n",
    "X_time12 = combined_df[AU_time12].values\n",
    "X_time13 = combined_df[BZGSM_IS1_time13].values\n",
    "X_time14 = combined_df[TEMP_IS1_time14].values\n",
    "X_time15 = combined_df[PDYN_IS1_time15].values\n",
    "X_time16 = combined_df[F107_IS1_time16].values\n",
    "\n",
    "\n",
    "X_time = np.stack((X_time1, X_time2,X_time3,X_time4,X_time5,X_time6,X_time7,X_time8,X_time9,X_time10,X_time11,X_time12,X_time13,X_time14,X_time15,X_time16), axis=2)\n",
    "print(X_time.shape)\n",
    "# 定义非时间序列特征列\n",
    "non_time_features = [ 'MLT','Interpolated_Lstar', 'PitchAngle','MLAT']\n",
    "\n",
    "# 提取非时间序列数据\n",
    "X_non_time = combined_df[non_time_features].values  # 形状 (num_samples, 3)\n",
    "y = np.log10(combined_df['Flux'].values+0.5 ) # 形状 (num_samples,)\n",
    "\n",
    "# 如果是二分类任务，将其转换为二维数组\n",
    "y = y.reshape(-1, 1)  # 形状 (num_samples, 1)\n",
    "flux_error = combined_df['Flux_Error'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_time = MinMaxScaler()\n",
    "num_samples = X_time.shape[0]\n",
    "time_steps = 151  # 根据您的数据调整\n",
    "num_features = 16  # 您有16个时间序列特征\n",
    "X_time_reshaped = X_time.reshape(-1, num_features)  # (num_samples * time_steps, 16)\n",
    "X_time_scaled = scaler_time.fit_transform(X_time_reshaped)\n",
    "X_time = X_time_scaled.reshape(num_samples, time_steps, num_features)\n",
    "scaler_non_time = StandardScaler()\n",
    "X_non_time = scaler_non_time.fit_transform(X_non_time)\n",
    "scaler_target = MinMaxScaler()\n",
    "y_scaled = scaler_target.fit_transform(y)\n",
    "\n",
    "\n",
    "# 归一化 Flux_Error\n",
    "scaler_flux = MinMaxScaler()\n",
    "flux_error_scaled = scaler_flux.fit_transform(flux_error)  # 缩放到 [0, 1]\n",
    "\n",
    "# 反转权重（假设较高的 Flux_Error 表示较低的质量）\n",
    "flux_error_reversed = 1 - flux_error_scaled \n",
    "min_weight = 0.2\n",
    "flux_error_final = flux_error_reversed * (1 - min_weight) + min_weight  # 缩放到 [min_weight, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7359075670283322 1.9031631163287668\n",
      "-1.8600403114495134\n"
     ]
    }
   ],
   "source": [
    "print(X_non_time[:,2].min(), X_non_time[:,2].max())\n",
    "original_value = 2.6\n",
    "\n",
    "# 使用 scaler_non_time 转换原始值为标准化后的值\n",
    "scaled_value = scaler_non_time.transform([[0, 0, original_value, 0]])\n",
    "\n",
    "L_YZ=scaled_value[0,2]\n",
    "print(L_YZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_time_train, X_time_val, X_non_time_train, X_non_time_val, y_train, y_val, weight_train, weight_val = train_test_split(\n",
    "    X_time, X_non_time, y_scaled, flux_error_final, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入模型\n",
    "model = tf.keras.models.load_model('LSTM_HIGH_3_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7699/7699 [==============================] - 25s 3ms/step\n",
      "测试集上的 RMSE: 0.1819352454861491 测试集上的 R2: 0.960436161518466\n",
      "测试集上的 RMSE_10: 32.50627348297522\n",
      "预测效率 (PE): 0.960436161518466\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "# 显示负号\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "mask = X_non_time_val[:,2] < L_YZ\n",
    "X_non_time_val_out = X_non_time_val[~mask]\n",
    "X_time_val_out = X_time_val[~mask]\n",
    "y_val_out = y_val[~mask]\n",
    "\n",
    "y_pred_scaled = model.predict([X_time_val_out, X_non_time_val_out])\n",
    "\n",
    "# 反缩放预测结果\n",
    "y_pred = scaler_target.inverse_transform(y_pred_scaled)\n",
    "y_test_subset = scaler_target.inverse_transform(y_val_out)\n",
    "# 生成布尔掩码\n",
    "\n",
    "\n",
    "y_test_exp = y_test_subset.flatten()\n",
    "y_pred_exp = y_pred.flatten()\n",
    "\n",
    "# 计算均方根误差（RMSE）\n",
    "rmse = np.sqrt(mean_squared_error(y_test_exp, y_pred_exp))\n",
    "r2 = r2_score(y_test_subset, y_pred)\n",
    "print(f'测试集上的 RMSE: {rmse}',f'测试集上的 R2: {r2}')\n",
    "\n",
    "y_test_10 = 10**y_test_exp\n",
    "y_pred_10 = 10**y_pred_exp\n",
    "rmse_10 = np.sqrt(mean_squared_error(y_test_10, y_pred_10)) \n",
    "print(f'测试集上的 RMSE_10: {rmse_10}')\n",
    "\n",
    "# 计算PE\n",
    "observed_values = y_test_exp  # 真实值\n",
    "predicted_values = y_pred_exp  # 预测值\n",
    "mean_observed = np.mean(observed_values)  # 真实值的均值\n",
    "\n",
    "# 计算PE\n",
    "numerator = np.sum((observed_values - predicted_values) ** 2)\n",
    "denominator = np.sum((observed_values - mean_observed) ** 2)\n",
    "\n",
    "pe = 1 - (numerator / denominator)\n",
    "\n",
    "print(f'预测效率 (PE): {pe}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('high_3_y_test_exp.txt', y_test_exp)\n",
    "np.savetxt('high_3_y_pred_exp.txt', y_pred_exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
