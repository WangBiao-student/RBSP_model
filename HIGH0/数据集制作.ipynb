{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import warnings\n",
    "from spacepy import pycdf\n",
    "from spacepy.time import Ticktock\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacepy import pycdf\n",
    "from spacepy.time import Ticktock\n",
    "from scipy import stats\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_cdf_file(file_path):\n",
    "    with pycdf.CDF(file_path) as cdf:\n",
    "        # 读取时间变量，并将其转换为 datetime 对象\n",
    "        epoch = cdf['Epoch'][...]\n",
    "        tt = Ticktock(epoch, 'CDFepoch')\n",
    "        datetime_array = np.array(tt.UTC)\n",
    "\n",
    "        # 读取能量标签\n",
    "        FEDU_ENERGY_LABL = cdf['FEDU_ENERGY_LABL'][...]  # 形状：[能量]\n",
    "        FEDU_ENERGY_LABL = np.array(FEDU_ENERGY_LABL).astype(str)  # 转换为字符串数组\n",
    "        FEDU_ENERGY_LABL = FEDU_ENERGY_LABL.squeeze()\n",
    "        # 去除前后空格并转换为大写\n",
    "        FEDU_ENERGY_LABL_clean = np.char.strip(FEDU_ENERGY_LABL)\n",
    "        FEDU_ENERGY_LABL_upper = np.char.upper(FEDU_ENERGY_LABL_clean)\n",
    "\n",
    "        # 查找包含 'LOW/2' 的能量标签（不区分大小写）\n",
    "        target_label = 'HIGH/0'############################################################################################################################################################################\n",
    "        contains_target = np.char.find(FEDU_ENERGY_LABL_upper, target_label.upper()) >= 0\n",
    "        energy_indices = np.where(contains_target)[0]\n",
    "        print(energy_indices)\n",
    "\n",
    "        if len(energy_indices) == 0:\n",
    "            print(f\"在文件 {file_path} 中未找到包含 '{target_label}' 的能量标签\")\n",
    "            return pd.DataFrame()  # 返回空的 DataFrame\n",
    "        else:\n",
    "            energy_index = energy_indices[0]  # 获取第一个匹配的索引\n",
    "\n",
    "        # 读取通量数据和相关变量\n",
    "        fedu_corr = cdf['FEDU_CORR'][...]  # 形状：[时间，俯仰角，能量]\n",
    "        fedu_corr_error = cdf['FEDU_CORR_ERROR'][...]\n",
    "        fedu_alpha = cdf['FEDU_Alpha'][...]  # 俯仰角数组\n",
    "\n",
    "        # 读取其他所需的变量\n",
    "        B_Calc = cdf['B_Calc'][...]\n",
    "        B_Eq = cdf['B_Eq'][...]\n",
    "        I = cdf['I'][...]\n",
    "        L = cdf['L'][...]\n",
    "        L_star = cdf['L_star'][...]\n",
    "        Position = cdf['Position'][...]\n",
    "        MLT = cdf['MLT'][...]\n",
    "        # 读取 LstarVsAlpha 和 LstarVsAlpha_Alpha\n",
    "        LstarVsAlpha = cdf['LstarVsAlpha'][...]          # 形状：[时间，18]\n",
    "        # LstarVsAlpha_Alpha = cdf['LstarVsAlpha_Alpha'][...]  # 形状：[18]\n",
    "\n",
    "    # 获取数据的维度\n",
    "    num_times = len(datetime_array)\n",
    "    num_pitches = len(fedu_alpha)\n",
    "    # 计算总记录数\n",
    "    total_records = num_times * num_pitches\n",
    "    # 创建索引网格，用于展开数据\n",
    "    time_indices = np.arange(num_times)\n",
    "    pitch_indices = np.arange(num_pitches)\n",
    "    time_grid, pitch_grid = np.meshgrid(\n",
    "        time_indices, pitch_indices, indexing='ij')\n",
    "\n",
    "    # 将网格展开为一维数组\n",
    "    time_flat = time_grid.flatten()\n",
    "    pitch_flat = pitch_grid.flatten()\n",
    "\n",
    "    # 提取并展开数据，以匹配总记录数\n",
    "    time_array = datetime_array[time_flat]\n",
    "    flux_array = fedu_corr[time_flat, pitch_flat, energy_index]\n",
    "    flux_error_array = fedu_corr_error[time_flat, pitch_flat, energy_index]\n",
    "    pitch_angle_array = fedu_alpha[pitch_flat]\n",
    "    B_Calc_array = B_Calc[time_flat]\n",
    "    B_Eq_array = B_Eq[time_flat]\n",
    "    I_array = I[time_flat]\n",
    "    L_array = L[time_flat]\n",
    "    L_star_array = L_star[time_flat]\n",
    "    MLT_array = MLT[time_flat]\n",
    "    Position_array = Position[time_flat, :]\n",
    "    X_array = Position_array[:, 0]\n",
    "    Y_array = Position_array[:, 1]\n",
    "    Z_array = Position_array[:, 2]\n",
    "    LstarVsAlpha90 = LstarVsAlpha[:,0]\n",
    "    LstarVsAlpha90=np.repeat(LstarVsAlpha90,11)\n",
    "    LstarVsAlpha85 = LstarVsAlpha[:,1]\n",
    "    LstarVsAlpha85=np.repeat(LstarVsAlpha85,11)\n",
    "    LstarVsAlpha80 = LstarVsAlpha[:,2]\n",
    "    LstarVsAlpha80=np.repeat(LstarVsAlpha80,11)\n",
    "    LstarVsAlpha75 = LstarVsAlpha[:,3]\n",
    "    LstarVsAlpha75=np.repeat(LstarVsAlpha75,11)\n",
    "    LstarVsAlpha70 = LstarVsAlpha[:,4]\n",
    "    LstarVsAlpha70=np.repeat(LstarVsAlpha70,11)\n",
    "    LstarVsAlpha65 = LstarVsAlpha[:,5]\n",
    "    LstarVsAlpha65=np.repeat(LstarVsAlpha65,11)\n",
    "    LstarVsAlpha60 = LstarVsAlpha[:,6]\n",
    "    LstarVsAlpha60=np.repeat(LstarVsAlpha60,11)\n",
    "    LstarVsAlpha55 = LstarVsAlpha[:,7]\n",
    "    LstarVsAlpha55=np.repeat(LstarVsAlpha55,11)\n",
    "    LstarVsAlpha50 = LstarVsAlpha[:,8]\n",
    "    LstarVsAlpha50=np.repeat(LstarVsAlpha50,11)\n",
    "    LstarVsAlpha45 = LstarVsAlpha[:,9]\n",
    "    LstarVsAlpha45=np.repeat(LstarVsAlpha45,11)\n",
    "    LstarVsAlpha40 = LstarVsAlpha[:,10]\n",
    "    LstarVsAlpha40=np.repeat(LstarVsAlpha40,11)\n",
    "    LstarVsAlpha35 = LstarVsAlpha[:,11]\n",
    "    LstarVsAlpha35=np.repeat(LstarVsAlpha35,11)\n",
    "    LstarVsAlpha30 = LstarVsAlpha[:,12]\n",
    "    LstarVsAlpha30=np.repeat(LstarVsAlpha30,11)\n",
    "    LstarVsAlpha25 = LstarVsAlpha[:,13]\n",
    "    LstarVsAlpha25=np.repeat(LstarVsAlpha25,11)\n",
    "    LstarVsAlpha20 = LstarVsAlpha[:,14]\n",
    "    LstarVsAlpha20=np.repeat(LstarVsAlpha20,11)\n",
    "    LstarVsAlpha15 = LstarVsAlpha[:,15]\n",
    "    LstarVsAlpha15=np.repeat(LstarVsAlpha15,11)\n",
    "    LstarVsAlpha10 = LstarVsAlpha[:,16]\n",
    "    LstarVsAlpha10=np.repeat(LstarVsAlpha10,11)\n",
    "    LstarVsAlpha5 = LstarVsAlpha[:,17]\n",
    "    LstarVsAlpha5=np.repeat(LstarVsAlpha5,11)\n",
    "\n",
    "    data = {\n",
    "        'Time': time_array,\n",
    "        'Flux': flux_array,\n",
    "        'Flux_Error': flux_error_array,\n",
    "        'PitchAngle': pitch_angle_array,\n",
    "        'B_Calc': B_Calc_array,\n",
    "        'B_Eq': B_Eq_array,\n",
    "        'I': I_array,\n",
    "        'L': L_array,\n",
    "        'L_star': L_star_array,\n",
    "        # 'L_star_PitchAngle': L_star_PitchAngle_array,\n",
    "        'MLT': MLT_array,\n",
    "        'X': X_array,\n",
    "        'Y': Y_array,\n",
    "        'Z': Z_array,\n",
    "        'LstarVsAlpha90': LstarVsAlpha90,\n",
    "        'LstarVsAlpha85': LstarVsAlpha85,\n",
    "        'LstarVsAlpha80': LstarVsAlpha80,\n",
    "        'LstarVsAlpha75': LstarVsAlpha75,\n",
    "        'LstarVsAlpha70': LstarVsAlpha70,\n",
    "        'LstarVsAlpha65': LstarVsAlpha65,\n",
    "        'LstarVsAlpha60': LstarVsAlpha60,\n",
    "        'LstarVsAlpha55': LstarVsAlpha55,\n",
    "        'LstarVsAlpha50': LstarVsAlpha50,\n",
    "        'LstarVsAlpha45': LstarVsAlpha45,\n",
    "        'LstarVsAlpha40': LstarVsAlpha40,\n",
    "        'LstarVsAlpha35': LstarVsAlpha35,\n",
    "        'LstarVsAlpha30': LstarVsAlpha30,\n",
    "        'LstarVsAlpha25': LstarVsAlpha25,\n",
    "        'LstarVsAlpha20': LstarVsAlpha20,\n",
    "        'LstarVsAlpha15': LstarVsAlpha15,\n",
    "        'LstarVsAlpha10': LstarVsAlpha10,\n",
    "        'LstarVsAlpha5': LstarVsAlpha5,\n",
    "    }\n",
    "\n",
    "    # 创建 DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # 过滤无效数据\n",
    "    df = df[df['Flux_Error'] >= 0]\n",
    "    # df = df[df['Flux_Error'] < 20]\n",
    "    df = df[df['B_Calc'] >= 0]\n",
    "    df = df[df['L_star'] >= 0]\n",
    "    df = df[df['Flux'] >=0]\n",
    "    df = df.sample(frac=0.005, replace=False, random_state=1565)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = ['../RBSP/A_L4', '../RBSP/B_L4']  # 请替换为您的文件夹路径\n",
    "\n",
    "# 存储所有 CDF 文件的路径\n",
    "cdf_files = []\n",
    "\n",
    "# 遍历所有文件夹，获取其中的 CDF 文件\n",
    "for folder_path in folder_paths:\n",
    "    cdf_files.extend([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.cdf')])\n",
    "\n",
    "print(len(cdf_files))\n",
    "\n",
    "#cdf_files=cdf_files[0:100]\n",
    "# 存储每个文件的 DataFrame\n",
    "df_list = []\n",
    "\n",
    "for file_path in cdf_files:\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    df = process_cdf_file(file_path)\n",
    "    if not df.empty:\n",
    "        df_list.append(df)\n",
    "    else:\n",
    "        print(f\"跳过文件 {file_path}，因为不包含目标能量标签。\")\n",
    "\n",
    "if df_list:\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    # 可选：查看拼接后的 DataFrame 的前几行\n",
    "    print(combined_df.head())\n",
    "else:\n",
    "    print(\"未找到包含目标能量标签的数据。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取列名中的角度信息\n",
    "angles = [int(col.split('LstarVsAlpha')[-1]) for col in combined_df.columns if 'LstarVsAlpha' in col]\n",
    "columns = [col for col in combined_df.columns if 'LstarVsAlpha' in col]\n",
    "\n",
    "# 将角度和列名对应关系构建成字典\n",
    "angle_column_map = dict(zip(angles, columns))\n",
    "\n",
    "# 对角度和列名按角度递增顺序排序\n",
    "sorted_pairs = sorted(zip(angles, columns))\n",
    "sorted_angles, sorted_columns = zip(*sorted_pairs)\n",
    "\n",
    "# 将 sorted_angles 和 sorted_columns 转换为列表\n",
    "sorted_angles = list(sorted_angles)\n",
    "sorted_columns = list(sorted_columns)\n",
    "\n",
    "# 将Lstar值中小于0的替换为NaN\n",
    "combined_df[sorted_columns] = combined_df[sorted_columns].applymap(lambda x: np.nan if x < 0 else x)\n",
    "\n",
    "# 定义插值函数\n",
    "def interpolate_row(row):\n",
    "    # 根据PitchAngle进行插值\n",
    "    target_angle = row['PitchAngle']\n",
    "    if target_angle > 90:\n",
    "        target_angle = 180 - target_angle\n",
    "\n",
    "    if target_angle < 5:\n",
    "        target_angle = 5\n",
    "\n",
    "    x = np.array(sorted_angles)\n",
    "    y = np.array([row[col] for col in sorted_columns])\n",
    "\n",
    "    # 筛选有效的（非NaN）数据点\n",
    "    valid_indices = ~np.isnan(y)\n",
    "    if np.sum(valid_indices) < 2:\n",
    "        # 若有效数据点少于2个，无法进行插值，返回NaN\n",
    "        return np.nan\n",
    "    else:\n",
    "        # 进行线性插值\n",
    "        return np.interp(target_angle, x[valid_indices], y[valid_indices])\n",
    "\n",
    "# 生成新列，存储插值后的结果\n",
    "combined_df['Interpolated_Lstar'] = combined_df.apply(interpolate_row, axis=1)\n",
    "\n",
    "# 显示结果\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=columns)\n",
    "#如果Interpolated_Lstar中有nan，用L_star列的值代替\n",
    "combined_df['Interpolated_Lstar'] = combined_df['Interpolated_Lstar'].fillna(combined_df['L_star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=1.064+0.511\n",
    "c=3*10**8\n",
    "m_0=0.51099895/(c**2)\n",
    "p=np.sqrt(E**2-0.511**2)/c\n",
    "# u=p**2/(2*m_0*207*10**-5)\n",
    "combined_df['u']=(p*np.sin(combined_df['PitchAngle']/180*np.pi))**2/(2*m_0*combined_df['B_Calc']*10**-5)\n",
    "print(combined_df['u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['PSD'] = combined_df['Flux']*1e3 / (2.9979e10*(p**2)*(c**2))\n",
    "print(combined_df['PSD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.sort_values('Time')\n",
    "combined_df = combined_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TQ=pd.read_csv('../空间天气参数/TQ.txt',sep='\\s+',header=None)\n",
    "TQ.columns=['year','day','hour','BXGSM','BYGSM','BZGSM','TEMP','DEN','PDYN','Kp','Dst','F107','AE','AL','AU']\n",
    "TQ['datetime'] = pd.to_datetime(TQ['year'].astype(str) + TQ['day'].astype(str).str.zfill(3) + TQ['hour'].astype(str).str.zfill(2), format='%Y%j%H')\n",
    "TQ=TQ.drop(['year','day','hour'],axis=1)\n",
    "TQ = TQ.sort_values(by='datetime')\n",
    "TQ.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除TQ中的BXGSM列\n",
    "TQ=TQ.drop(['BXGSM'],axis=1)\n",
    "TQ=TQ.drop(['BYGSM'],axis=1)\n",
    "TQ=TQ.drop(['DEN'],axis=1)\n",
    "\n",
    "print(TQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = np.abs(stats.zscore(TQ['BZGSM']))\n",
    "threshold = 3  # 设置Z-score阈值\n",
    "TQ['BZGSM_IS1'] = np.where(z_scores >= threshold, 0, 1)\n",
    "TQ['BZGSM_no_outliers'] = TQ['BZGSM'].where(z_scores < threshold, np.nan)  # 将异常值替换为NaN\n",
    "\n",
    "# 在原位置插值\n",
    "TQ['BZGSM'] = TQ['BZGSM_no_outliers'].interpolate(method='time')\n",
    "# 删除临时列\n",
    "TQ = TQ.drop(columns=['BZGSM_no_outliers'])\n",
    "plt.plot(TQ.index, TQ['BZGSM'])\n",
    "\n",
    "z_scores = np.abs(stats.zscore(TQ['TEMP']))\n",
    "threshold = 3  # 设置Z-score阈值\n",
    "TQ['TEMP_IS1'] = np.where(z_scores >= threshold, 0, 1)\n",
    "TQ['TEMP_no_outliers'] = TQ['TEMP'].where(z_scores < threshold, np.nan)  # 将异常值替换为NaN\n",
    "\n",
    "# 在原位置插值\n",
    "TQ['TEMP'] = TQ['TEMP_no_outliers'].interpolate(method='time')\n",
    "# 删除临时列\n",
    "TQ = TQ.drop(columns=['TEMP_no_outliers'])\n",
    "plt.plot(TQ.index, TQ['TEMP'])\n",
    "\n",
    "z_scores = np.abs(stats.zscore(TQ['PDYN']))\n",
    "threshold = 3  # 设置Z-score阈值\n",
    "TQ['PDYN_IS1'] = np.where(z_scores >= threshold, 0, 1)\n",
    "TQ['PDYN_no_outliers'] = TQ['PDYN'].where(z_scores < threshold, np.nan)  # 将异常值替换为NaN\n",
    "\n",
    "# 在原位置插值\n",
    "TQ['PDYN'] = TQ['PDYN_no_outliers'].interpolate(method='time')\n",
    "# 删除临时列\n",
    "TQ = TQ.drop(columns=['PDYN_no_outliers'])\n",
    "plt.plot(TQ.index, TQ['PDYN'])\n",
    "\n",
    "z_scores = np.abs(stats.zscore(TQ['F107']))\n",
    "threshold = 3  # 设置Z-score阈值\n",
    "TQ['F107_IS1'] = np.where(z_scores >= threshold, 0, 1)\n",
    "TQ['F107_no_outliers'] = TQ['F107'].where(z_scores < threshold, np.nan)  # 将异常值替换为NaN\n",
    "\n",
    "# 在原位置插值\n",
    "TQ['F107'] = TQ['F107_no_outliers'].interpolate(method='time')\n",
    "# 删除临时列\n",
    "TQ = TQ.drop(columns=['F107_no_outliers'])\n",
    "plt.plot(TQ.index, TQ['F107'])\n",
    "\n",
    "V_H_5=pd.read_csv('../空间天气参数/V_H_5.txt',sep='\\s+',header=None)\n",
    "V_H_5.columns=['year','day','hour','min','VX', 'VY', 'VZ', 'SYMH']\n",
    "year_str = V_H_5['year'].astype(str)\n",
    "day_str = V_H_5['day'].astype(str).str.zfill(3)\n",
    "hour_str = V_H_5['hour'].astype(str).str.zfill(2)\n",
    "min_str = V_H_5['min'].astype(str).str.zfill(2)\n",
    "\n",
    "# 合并字符串\n",
    "datetime_str = year_str + day_str + hour_str + min_str\n",
    "\n",
    "# 转换为 datetime 对象\n",
    "V_H_5['datetime'] = pd.to_datetime(datetime_str, format='%Y%j%H%M')\n",
    "V_H_5=V_H_5.drop(['year','day','hour','min'],axis=1)\n",
    "V_H_5 = V_H_5.sort_values(by='datetime')\n",
    "V_H_5.set_index('datetime', inplace=True)\n",
    "\n",
    "V_H_5=V_H_5.drop(['VY'],axis=1)\n",
    "V_H_5=V_H_5.drop(['VZ'],axis=1)\n",
    "print(V_H_5)\n",
    "\n",
    "\n",
    "\n",
    "z_scores = np.abs(stats.zscore(V_H_5['VX']))\n",
    "threshold = 3  # 设置Z-score阈值\n",
    "V_H_5['VX_IS1'] = np.where(z_scores >= threshold, 0, 1)\n",
    "V_H_5['VX_no_outliers'] = V_H_5['VX'].where(z_scores < threshold, np.nan)  # 将异常值替换为NaN\n",
    "# 在原位置插值\n",
    "V_H_5['VX'] = V_H_5['VX_no_outliers'].interpolate(method='time')\n",
    "# 删除临时列\n",
    "V_H_5 = V_H_5.drop(columns=['VX_no_outliers'])\n",
    "\n",
    "\n",
    "plt.plot(V_H_5.index, V_H_5['SYMH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将V_H_5修改为1h的频率\n",
    "V_H_5=V_H_5.resample('1H').mean()\n",
    "print(V_H_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Time'] = pd.to_datetime(combined_df['Time'])\n",
    "combined_df = combined_df.sort_values('Time')\n",
    "combined_df = combined_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_df_2 = combined_df['Time'].copy()\n",
    "\n",
    "for i in range(0, 301):\n",
    "    # 创建新的时间列，向后偏移 i 小时\n",
    "    combined_df[f'datetime_minus'] = combined_df_2 - pd.Timedelta(hours=i)\n",
    "    \n",
    "    # 重命名列\n",
    "    \n",
    "    rename_TQ = {col: f\"{col}_{i}h\" for col in TQ.columns if col != 'datetime'}\n",
    "    rename_V_H_5 = {col: f\"{col}_{i}h\" for col in V_H_5.columns if col != 'datetime'}\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.merge_asof(\n",
    "        combined_df.sort_values(f'datetime_minus'),\n",
    "        V_H_5.rename(columns=rename_V_H_5).sort_values('datetime'),\n",
    "        left_on=f'datetime_minus', right_on='datetime',\n",
    "        direction='backward', tolerance=pd.Timedelta('1H')\n",
    "    )\n",
    "    combined_df = pd.merge_asof(\n",
    "        combined_df.sort_values(f'datetime_minus'),\n",
    "        TQ.rename(columns=rename_TQ).sort_values('datetime'),\n",
    "        left_on=f'datetime_minus', right_on='datetime',\n",
    "        direction='backward', tolerance=pd.Timedelta('1H')\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 删除多余的 'datetime' 列\n",
    "    combined_df.drop(columns=['datetime_minus'], inplace=True)\n",
    "    combined_df.rename(columns={'datetime_x': 'datetime'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_hdf('combined_df_test.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
